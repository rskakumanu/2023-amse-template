{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Data Pipeline Execution\n",
    "\n",
    "- Created a new folder named \"/data\" in the root of the project repository.\n",
    "- Developed a data engineering script to pull, clean, and store the data.\n",
    "- Placed the script in the \"/data\" directory.\n",
    "- Successfully executed the script, generating local datasets in the \"/data\" directory, stored as SQLite databases.\n",
    "- Ensured that the generated datasets are not checked into version control by updating the .gitignore file.\n",
    "- Reviewed and updated the project's issues and project plan as necessary.\n",
    "\n",
    "The automated data pipeline has been successfully implemented, enabling efficient data processing and storage for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import several libraries that will be used in our notebook\n",
    "\n",
    "- `pandas` is a powerful data manipulation library in Python.\n",
    "- `numpy` provides support for mathematical operations on arrays and matrices.\n",
    "- `sqlite3` is a module that provides a simple way to work with SQLite databases.\n",
    "- `sqlalchemy` is a SQL toolkit and Object-Relational Mapping (ORM) library.\n",
    "- `urllib.request` is used for opening URLs and downloading files.\n",
    "- `zipfile` is used for working with ZIP archives.\n",
    "- `kaggle` is a command-line tool for interacting with the Kaggle platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to extract data from a file and load it into a SQLite database table. It takes the file_name and table_name as input parameters. If the url_path is provided, it downloads the file from the URL before loading the data. The data is read from the file using pandas and then stored in a SQLite database table specified by table_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_load(file_name, table_name, url_path = None):\n",
    "    \"\"\"\n",
    "    Extracts data from a file and loads it into a SQLite database table.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: Name of the file to extract data from\n",
    "    - table_name: Name of the table to load data into\n",
    "    - url_path (optional): URL path to download the file from\n",
    "\n",
    "    If `url_path` is provided, the file will be downloaded before loading the data.\n",
    "\n",
    "    The data is read from the file using pandas and then stored in a SQLite database table.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if url_path:\n",
    "        urllib.request.urlretrieve(url_path, file_name)\n",
    "        data = pd.read_csv(file_name, delimiter=\";\", skip_blank_lines= True, on_bad_lines='skip', low_memory= False)\n",
    "    else:\n",
    "        data = pd.read_csv(file_name, delimiter=\",\")\n",
    "        \n",
    "    \n",
    "    blank_columns = []\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().all():\n",
    "            blank_columns.append(column)\n",
    "    \n",
    "    data.drop(blank_columns, axis=1, inplace=True)\n",
    "\n",
    "    conn = sqlite3.connect(\"data.sqlite\")\n",
    "    data.to_sql(table_name, conn, if_exists=\"replace\", index= False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code snippet extracts the speed monitoring data from the given URL (url_speed) and saves it to a file named file_speed. Then, it calls the extract_load function to load the data from the file into a SQLite database table named \"data_speed\". The extract_load function handles the extraction and loading process, utilizing the provided URL and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and load speed monitoring data into a SQLite database\n",
    "url_speed = \"https://offenedaten-koeln.de/sites/default/files/Geschwindigkeit%C3%BCberwachung_Koeln_Gesamt_2017-2022_0.csv\"\n",
    "file_speed = \"Geschwindigkeitüberwachung_Koeln_Gesamt_2017-2022_0.csv\"\n",
    "\n",
    "# load the speed monitoring data into the \"data_speed\" table\n",
    "extract_load(url_path=url_speed, file_name= file_speed, table_name = \"data_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the kaggle command-line tool to download the weather data from the specified Kaggle dataset. It then extracts the downloaded ZIP file, placing the extracted files in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weather-cologne-bonn-history.zip to d:\\myprojects\\fau\\saki\\2023-amse-template\\project\\data"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/11.3M [00:00<?, ?B/s]\n",
      "  9%|▉         | 1.00M/11.3M [00:00<00:01, 8.71MB/s]\n",
      " 18%|█▊        | 2.00M/11.3M [00:00<00:01, 8.27MB/s]\n",
      " 27%|██▋       | 3.00M/11.3M [00:00<00:00, 8.91MB/s]\n",
      " 35%|███▌      | 4.00M/11.3M [00:00<00:00, 8.51MB/s]\n",
      " 44%|████▍     | 5.00M/11.3M [00:00<00:00, 8.18MB/s]\n",
      " 53%|█████▎    | 6.00M/11.3M [00:00<00:00, 7.97MB/s]\n",
      " 62%|██████▏   | 7.00M/11.3M [00:00<00:00, 7.96MB/s]\n",
      " 71%|███████   | 8.00M/11.3M [00:01<00:00, 8.06MB/s]\n",
      " 80%|███████▉  | 9.00M/11.3M [00:01<00:00, 8.30MB/s]\n",
      " 88%|████████▊ | 10.0M/11.3M [00:01<00:00, 6.09MB/s]\n",
      "100%|██████████| 11.3M/11.3M [00:01<00:00, 7.22MB/s]\n",
      "100%|██████████| 11.3M/11.3M [00:01<00:00, 7.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and extract weather data from Kaggle\n",
    "!kaggle datasets download -d bastitee/weather-cologne-bonn-history\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "with zipfile.ZipFile(\"weather-cologne-bonn-history.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads the weather data from the specified CSV file into a SQLite database table named \"data_weather\". The extract_load function is called with the file name and table name as arguments, and it handles the extraction and loading process into the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data into SQLite database\n",
    "file_speed = \"weather-cologne-bonn-history-2000-2021.csv\"\n",
    "extract_load(file_name= file_speed, table_name = \"data_weather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
